{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Json to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json_to_csv(input_path,output_path):\n",
    "    file = open(input_path,encoding=\"utf8\")\n",
    "    data=json.load(file)\n",
    "    df = pd.read_json(data)\n",
    "    converted_frame = df.to_csv(output_path)   \n",
    "    file.close()\n",
    "    print(df)\n",
    "    return converted_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataframe from the elasticdump pulled json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_to_data_frame(input_path):\n",
    "    with open(input_path,\"r\",encoding=\"utf-8\") as data:\n",
    "        Lines = data.readlines()\n",
    "    print(Lines[0])\n",
    "    local_json = json.loads(Lines[0])\n",
    "    df = pd.read_json(local_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_name = Path('C:/Users/Itai Ofir/HebrewCourtVerdictsAnalyzer/ML/data/parts/veredicts00.json')\n",
    "#output_file_name = Path('C:/Users/Itai Ofir/HebrewCourtVerdictsAnalyzer/ML/data/veredicts.csv')\n",
    "df = load_json_to_data_frame(input_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the json to individual jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_elasticdump_json_to_individual_text_files(input_json_path, output_json_path):\n",
    "    with open(input_json_path,\"r\",encoding=\"utf-8\") as json_file:\n",
    "        Lines = json_file.readlines()   \n",
    "    for line in Lines:\n",
    "        line_as_string = str.format('{0}',line)\n",
    "        id = extract_id_from_string_json_parser(line_as_string)\n",
    "        str_end = id +\".json\"\n",
    "        current_path = Path(os.path.join(output_json_path,str_end))\n",
    "        with open(current_path,\"w\",encoding=\"utf-8\") as outfile:\n",
    "            outfile.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract id from one lined json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_id_from_string_json_parser(line):\n",
    "    json_dict = json.loads(line)\n",
    "    id = json_dict[\"_id\"]\n",
    "    return id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run to split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_name = Path('C:/Users/Itai Ofir/HebrewCourtVerdictsAnalyzer/ML/data/veredicts.json')\n",
    "output_directory = Path('C:/Users/Itai Ofir/HebrewCourtVerdictsAnalyzer/ML/data/splitteredjsonsaftercorrection')\n",
    "\n",
    "split_elasticdump_json_to_individual_text_files(input_file_name,output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract summary section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_string(path):\n",
    "    with open(path,\"r\",encoding=\"utf-8\") as json_file:\n",
    "        json_dict = json.load(json_file)\n",
    "        summary = json_dict[\"_source\"][\"doc\"][\"Doc Details\"][\"סיכום\"]\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if input has requested keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_input_string_has_keywords(input_string, keywords):\n",
    "    for key in keywords:\n",
    "        if(input_string.find(key) != -1):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('C:/Users/Itai Ofir/HebrewCourtVerdictsAnalyzer/ML/data/splittedjsons/000-01-2.json')\n",
    "summary_string = get_summary_string(path)\n",
    "print(summary_string)\n",
    "print(check_if_input_string_has_keywords(summary_string,[\"פירעון\",\"במבה\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look for each and every json file and look for keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_how_many_keywords_instances(path,keywords,output_path):\n",
    "    matches = 0\n",
    "    for file in os.scandir(path):\n",
    "        current_summary = get_summary_string(file.path)            \n",
    "        if check_if_input_string_has_keywords(current_summary,keywords):\n",
    "            shutil.copy(file.path,output_path)  \n",
    "            matches += 1\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('C:/Users/Itai Ofir/HebrewCourtVerdictsAnalyzer/ML/data/splitteredjsonsaftercorrection')\n",
    "output_path = Path('C:/Users/Itai Ofir/HebrewCourtVerdictsAnalyzer/ML/data/realestateaftercorrection')\n",
    "keywords = [\"בניין\",\"מקרקעין\",\"קרקע\",\"בנין\",\"נדלן\",\"אדמה\",\"שטח\",\"התיישבות\"]\n",
    "count = check_how_many_keywords_instances(path,keywords,output_path)\n",
    "print (count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataframe for more accsessibility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(input_data,required_instances_count):\n",
    "    df = pd.DataFrame(data=None,index=input_data.index,columun=input_data.columun)\n",
    "    count = 0\n",
    "    for row in data.index:\n",
    "        df.append(row)\n",
    "        count+=1\n",
    "        if (count == required_instances_count):\n",
    "            break\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('C:/Users/Itai Ofir/HebrewCourtVerdictsAnalyzer/ML/data/parts/veredicts00.json')\n",
    "df\n",
    "#splitted_df = split_dataframe(df,100)\n",
    "#splitted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing the conclusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_quanitty_of_instances(input):\n",
    "    counts = dict()\n",
    "    words = re.sub('[^אבגדהוזחטיכלמנסעפצקרשתןףךץם]',' ',input)\n",
    "    words = words.split()\n",
    "    for word in words:\n",
    "        if len(word) > 1:\n",
    "            if word in counts:\n",
    "                counts[word] += 1\n",
    "            else:\n",
    "                counts[word] = 1\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_quanitty_of_instances_DF(input):\n",
    "    counts = pd.DataFrame(columns=[\"Word\",\"Count\"])\n",
    "    words = re.sub('[^אבגדהוזחטיכלמנסעפצקרשתןףךץם]',' ',input)\n",
    "    words = words.split()\n",
    "    lst = []\n",
    "    for word in words:\n",
    "        if len(word) > 1:\n",
    "            if word in counts:\n",
    "                if(counts[word] != null):\n",
    "                    counts.loc[\"Word\"] += 1\n",
    "                else:\n",
    "                    lst = lst.apppend([word,1])\n",
    "                    counts = counts.append(lst)\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_test = str(\"2. בית המשפט המחוזי דחה את העתירה משסבר כי נוכח התפרצותו המחודשת של וירוס הקורונה, אין מקום להתערב בהחלטת הממונה על ניהול בתי הסוהר להוסיף ולהטיל מגבלה על ביקורי התייחדות - בהיותו הגורם המוסמך על פי החוק שהביא את ההמלצות והנתונים של משרד הבריאות במכלול שיקוליו. בנוסף הודגש, כי מדובר בהגבלה החלה על קבוצת אסירים גדולה ואין עניינו של המבקש שונה משל אסירים אחרים. לצד זאת הובהר כי במקרה דנן הדברים מקבלים משנה תוקף, משום שגורמי שב\\\"ס הציעו למבקש מנגנון חלופי שיאפשר לו ולאשתו לנסות ולהביא ילד לעולם, ללא צורך בביקורי התייחדות, אך הצעות אלו נדחו על-ידי המבקש.\\n3. לאחר עיון הגעתי למסקנה כי דין הבקשה להידחות.\\n4. כידוע בית משפט זה ייעתר לבקשת רשות ערעור על פסק דין שניתן בעתירת אסיר במקרים חריגים בהם מתעוררת שאלה משפטית עקרונית או סוגיה בעלת חשיבות כללית, החורגת מעניינם הפרטני של הצדדים להליך (רע\\\"ב 7/86 וייל נ' מדינת ישראל (26.6.1986)), המקרה דנן אינו בא בגדרה של אמת מידה מחמירה זו.\\n5. אשר על כן, הבקשה נדחית.\\nניתנה היום, כ' באב התש\\\"ף (10.8.2020).\",)\n",
    "#summary_test\n",
    "dict_after = determine_quanitty_of_instances(summary_test)\n",
    "#dict_after.sort(key=lambda x: x[1],ascending=False)\n",
    "dict_after\n",
    "#max(dict_after.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('C:/Users/Itai Ofir/HebrewCourtVerdictsAnalyzer/ML/data/test.json')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract id, veredict from specified json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_id_and_veredict_from_specified_json(input_json_path):\n",
    "    with open(input_json_path,\"r\",encoding=\"utf-8\") as json_file:\n",
    "        json_dict = json.load(json_file)\n",
    "        Veredict = json_dict[\"_source\"][\"doc\"][\"Doc Details\"][\"סיכום\"]\n",
    "        ID = json_dict[\"_id\"]\n",
    "    return ID,Veredict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing the conclusion with sklearn CountVecotrizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('C:/Users/Itai Ofir/HebrewCourtVerdictsAnalyzer/ML/data/realestate/0001-08-1.json')\n",
    "ID,Veredict = extract_id_and_veredict_from_specified_json(path)\n",
    "df = pd.DataFrame([[ID,Veredict,0]],columns=[\"ID\",\"Veredict\",\"Category\"])\n",
    "vectorizer = TfidfVectorizer()\n",
    "freq_vec = vectorizer.fit_transform(df[\"Veredict\"]).toarray()\n",
    "freqDist=dict(zip(vectorizer.get_feature_names(),freq_vec[0])) # create a dictionary from the first (and only) row\n",
    "freqDist=dict(sorted(freqDist.items(), key=lambda item: item[1],reverse=True)) # ascending sort..\n",
    "topK=30\n",
    "\n",
    "keys_to_remove = []\n",
    "count=0\n",
    "for key in freqDist:\n",
    "    if(count < 30):\n",
    "        keys_to_remove.append(key)\n",
    "        count += 1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "for key in keys_to_remove:\n",
    "    del freqDist[key]\n",
    "\n",
    "plt.bar(list(freqDist.keys())[:topK], list(freqDist.values())[:topK])\n",
    "plt.xticks(list(freqDist.keys())[:topK], rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "#tokenized = determine_quanitty_of_instances(Veredict)\n",
    "#tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check latest and earliest veredict exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_latest_and_earliest_veredict(directory):\n",
    "    earliest = datetime.date.today()\n",
    "    latest = datetime.date(1, 1, 1)\n",
    "    currupt_json_id_list = []\n",
    "    count = 0\n",
    "    print(\"checking..\")\n",
    "    for file in os.scandir(directory):\n",
    "        with open(file,\"r\",encoding=\"utf-8\") as json_file:\n",
    "            cur = json.load(json_file)\n",
    "            cur_date = cur[\"_source\"][\"doc\"][\"Doc Details\"][\"תאריך\"]\n",
    "            if(cur_date != None):\n",
    "                splitted_date = cur_date.split('/')\n",
    "                cur_day = (int)(splitted_date[0])\n",
    "                cur_month = (int)(splitted_date[1])\n",
    "                cur_year = (int)(splitted_date[2])\n",
    "                cur_datetime = datetime.date(cur_year,cur_month,cur_day)\n",
    "            \n",
    "                if cur_datetime > latest:\n",
    "                    latest = cur_datetime\n",
    "                \n",
    "                if cur_datetime < earliest:\n",
    "                    earliest = cur_datetime   \n",
    "            \n",
    "                count += 1\n",
    "                if(count%10000 == 0):\n",
    "                    print('Scanned ' + str(count) + ' verdicts')\n",
    "            else:\n",
    "                currupt_json_id_list.append(cur[\"_id\"])\n",
    "    print('Total scanned ' + str(count) + ' verdicts')\n",
    "    \n",
    "    return earliest,latest,currupt_json_id_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earliest,latest,corrupt_list = determine_latest_and_earliest_veredict('C:/Users/Itai Ofir/HebrewCourtVerdictsAnalyzer/ML/data/splitteredjsonsaftercorrection')\n",
    "print(earliest)\n",
    "print(latest)\n",
    "print(corrupt_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy verdicts to category folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_verdicts_and_sort_by_category_using_csv(csv_file_path,verdicts_path,destination_path):\n",
    "    with open(csv_file_path, 'r') as csvfile:\n",
    "        datareader = csv.reader(csvfile)\n",
    "        rows_scanned = 0\n",
    "        for row in datareader: #each row yields a file transfer\n",
    "            try:\n",
    "                rows_count = 0\n",
    "                csv_id = row[0]\n",
    "                csv_splitted_id = csv_id.split(\"/\")\n",
    "                first_category = row[1]\n",
    "                seceond_category = row[2]\n",
    "                for verdict in os.scandir(verdicts_path): # each veredict can be picked once and only once \n",
    "                    with open(verdict,\"r\",encoding=\"utf-8\") as json_file:\n",
    "                        cur = json.load(json_file)\n",
    "                        cur_id = cur[\"_id\"]\n",
    "                        verdict_splitted_cur_id = cur_id.split(\"-\")\n",
    "                        num_of_categories_count = 1\n",
    "                        if len(csv_splitted_id[0]) == len(verdict_splitted_cur_id[0]) and len(csv_splitted_id[1]) == len(verdict_splitted_cur_id[1]):\n",
    "                            if csv_splitted_id[0] == verdict_splitted_cur_id[0] and csv_splitted_id[1] == verdict_splitted_cur_id[1]: #discussion number is not relevant\n",
    "                                while num_of_categories_count <= 2:\n",
    "                        #need to check if directory exists\n",
    "                        # yes - just copy\n",
    "                        # no- create the directory and then copy\n",
    "                                    if row[num_of_categories_count] != None:\n",
    "                                        cur_category_path = destination_path + \"/\" + row[num_of_categories_count]\n",
    "                                        pathlib.Path(cur_category_path).mkdir(parents=True, exist_ok=True)\n",
    "                        #copy code here\n",
    "                                        shutil.copy(verdict,cur_category_path)\n",
    "                                        num_of_categories_count += 1\n",
    "                               \n",
    "                                    else:\n",
    "                                        break\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                    \n",
    "            rows_scanned += 1\n",
    "            print('Scanned ' + str(rows_scanned) + ' rows')\n",
    "    # foreach row check if directory already exists on destination\n",
    "    # yes - look for id on verdicts path and copy\n",
    "    # no - create and then do the same\n",
    "    # result - all json's ready to be eaten by classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = 'C:/Users/Itai Ofir/HebrewCourtVerdictsAnalyzer/ML/VeredictByCategory.csv'\n",
    "verdicts_path = 'C:/Users/Itai Ofir/HebrewCourtVerdictsAnalyzer/ML/data/splitteredjsonsaftercorrection'\n",
    "dest_path = 'C:/Users/Itai Ofir/HebrewCourtVerdictsAnalyzer/ML/data/readytoclassify'\n",
    "copy_verdicts_and_sort_by_category_using_csv(csv_path,verdicts_path,dest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
