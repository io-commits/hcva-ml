{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import jellyfish\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from itertools import repeat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A method to pull the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_doc_details_values_list(input_directory,output_directory):\n",
    "    \n",
    "#     \"\"\"\n",
    "#     Pulls all of the naming existing on the verdicts jsons and stores them in sets.\n",
    "    \n",
    "#     Get the verdicts location as input directory and the output folder which will \n",
    "#     store the sets as txt file with the names seperated by '*'.\n",
    "    \n",
    "#     input_directory - path as a string\n",
    "#     output_directory - path as a string\n",
    "    \n",
    "#     returnes all the sets created one by one.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # set initialization\n",
    "#     judge_set = set()\n",
    "#     petitioner_set = set()\n",
    "#     defendse_set = set()\n",
    "#     petitioner_attorney_set = set()\n",
    "#     defendse_attorney_set = set()\n",
    "    \n",
    "#     # populating the sets\n",
    "#     for verdict in os.scandir(input_directory):\n",
    "#         with open(verdict,\"r\",encoding=\"utf-8\") as verdict_file:\n",
    "#             verdict_json = json.load(verdict_file)\n",
    "            \n",
    "#             judges = verdict_json[\"_source\"][\"doc\"][\"Doc Details\"][\"לפני\"]\n",
    "#             if(judges != None):\n",
    "#                 for judge in judges:\n",
    "#                     judge_set.add(judge)\n",
    "            \n",
    "#             petitioners = verdict_json[\"_source\"][\"doc\"][\"Doc Details\"][\"העותר\"]\n",
    "#             if(petitioners != None):\n",
    "#                 for petitioner in petitioners:\n",
    "#                     petitioner_set.add(petitioner)\n",
    "            \n",
    "#             defense = verdict_json[\"_source\"][\"doc\"][\"Doc Details\"][\"המשיב\"]\n",
    "#             if(defense != None):\n",
    "#                 for defendee in defense:\n",
    "#                     defendse_set.add(defendee)\n",
    "                    \n",
    "#             petitioner_attorneys = verdict_json[\"_source\"][\"doc\"][\"Doc Details\"][\"בשם העותר\"]\n",
    "#             if(petitioner_attorneys != None):\n",
    "#                 for petitioner_attorney in petitioner_attorneys:\n",
    "#                     petitioner_attorney_set.add(petitioner_attorney)\n",
    "                    \n",
    "#             defendse_attorney = verdict_json[\"_source\"][\"doc\"][\"Doc Details\"][\"בשם המשיב\"]\n",
    "#             if(defendse_attorney != None):\n",
    "#                 for def_attorney in defendse_attorney:\n",
    "#                     defendse_attorney_set.add(def_attorney)\n",
    "    \n",
    "#     # creating the files\n",
    "#     cur_path = output_directory + \"/\" + 'judges.txt'\n",
    "#     with open(cur_path,\"w\",encoding=\"utf-8\") as judges_file:\n",
    "#          for judge in judge_set:\n",
    "#             judges_file.write(judge + \"*\")\n",
    "        \n",
    "#     cur_path = output_directory + \"/\" + 'petitioner.txt'\n",
    "#     with open(cur_path,\"w\",encoding=\"utf-8\") as petitioner_file:\n",
    "#         for petitioner in petitioner_set:\n",
    "#               petitioner_file.write(petitioner + \"*\")\n",
    "        \n",
    "#     cur_path = output_directory + \"/\" + 'defense.txt'\n",
    "#     with open(cur_path,\"w\",encoding=\"utf-8\") as defense_file:\n",
    "#         for defense in defendse_set:\n",
    "#             defense_file.write(defense + \"*\")\n",
    "                \n",
    "#     cur_path = output_directory + \"/\" + 'petitioner_attorneys.txt'\n",
    "#     with open(cur_path,\"w\",encoding=\"utf-8\") as petitioner_attorney_file:\n",
    "#         for petitioner_attorney in petitioner_attorney_set:\n",
    "#                 petitioner_attorney_file.write(petitioner_attorney + \"*\")\n",
    "                \n",
    "#     cur_path = output_directory + \"/\" + 'defendse_attorney.txt'\n",
    "#     with open(cur_path,\"w\",encoding=\"utf-8\") as defendse_attorney_file:\n",
    "#         for defendse_attorney in defendse_attorney_set:\n",
    "#             defendse_attorney_file.write(defendse_attorney + \"*\")\n",
    "                \n",
    "#     return judge_set,petitioner_set,defendse_set,petitioner_attorney_set,defendse_attorney_set\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verdicts_path = 'C:/Users/Itai Ofir/HebrewCourtVerdictsAnalyzer/ML/data/splitteredjsonsaftercorrection'\n",
    "# files_directory = 'C:/Users/Itai Ofir/HebrewCourtVerdictsAnalyzer/ML/data/normalization'\n",
    "# a,b,c,d,e = make_doc_details_values_list(verdicts_path,files_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_naming_df(path):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Read txt file with names serperated by '*'.\n",
    "    \n",
    "    Populating a pandas dataframe with the names as well as the role, for example: \n",
    "    אסתר חיות - judges\n",
    "    \n",
    "    input - the txt file path\n",
    "    \n",
    "    returns the newly created df\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #check if path exists\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        \n",
    "        # initialize lists\n",
    "        \n",
    "        names = list()\n",
    "        roles = list()\n",
    "        \n",
    "        # iterate on the files\n",
    "        for file in os.scandir(path):\n",
    "            \n",
    "            # populate lists\n",
    "            with open(file,\"r\",encoding=\"utf-8\") as cur_file:\n",
    "                \n",
    "                # variable contains the number of names on the examined role\n",
    "                before_assignment_count = len(names)\n",
    "                \n",
    "                # read the whole file\n",
    "                cur_str = cur_file.read()\n",
    "                \n",
    "                # get only file name without the format\n",
    "                cur_role = Path(file).name.split(\".\")[0]\n",
    "                \n",
    "                # append all the names to the current list\n",
    "                names = names + (cur_str.split(\"*\"))\n",
    "                cur_list = list()\n",
    "                \n",
    "                # calculate how many names exists on the current iteration and extend roles list accoridngly\n",
    "                cur_list.extend(repeat(cur_role,(len(names)-before_assignment_count)))\n",
    "                roles = roles + cur_list\n",
    "                print(str.format(\"{0} - {1}\",cur_role,str(len(names)-before_assignment_count)))           \n",
    "        \n",
    "        # populate the df with the resulting lists\n",
    "        df = pd.DataFrame()\n",
    "        df[\"Before\"] = names\n",
    "        df[\"Role\"] = roles\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic regex rephraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_words_corresponds_to_regex_list(rgx_list, replace_with_that_str, input_text):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    rgx_list - list of patterns\n",
    "    \n",
    "    replace_with_that_str - str to replace when match exists\n",
    "    \n",
    "    input_text - text to look on for patterns\n",
    "    \n",
    "    returns the new text after subtracting the matches\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    new_text = input_text\n",
    "    for rgx_match in rgx_list:\n",
    "        new_text = re.sub(rgx_match, replace_with_that_char, new_text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex rephraser for naming lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_regex_rules_on_naming_csv(replace_with_this_str:str, input_name:str, csv_path:str):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Adds to every name hebrew prefixes that might exists on the data.\n",
    "    \n",
    "    This method is very expansive and not recommended when not necessary.\n",
    "    \n",
    "    \n",
    "    \n",
    "    replace_with_this_str - the str tobe replaced with\n",
    "    \n",
    "    input_name - name string\n",
    "    \n",
    "    csv_path - the csv that contains the names to be subtracted from each and every matched column\n",
    "    \n",
    "    \n",
    "    returns the element after subtracting matched words\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # initalization\n",
    "    updated_name = input_name\n",
    "    names_list=list()\n",
    "    rgx_list=list()\n",
    "    \n",
    "    #open the csv file with the words to eliminate\n",
    "    with open(csv_path, \"r\", encoding = 'utf-8') as csv_file:\n",
    "        \n",
    "        names_list = csv.reader(csv_file,delimiter=',')\n",
    "                \n",
    "        names = [row[0] for row in names_list]\n",
    "        \n",
    "        # add selcted prefixes\n",
    "        for name in names:\n",
    "            rgx_list.append(str.format(\"ו?\"+name))\n",
    "            rgx_list.append(str.format(\"ל?\"+name))\n",
    "            rgx_list.append(str.format(\"כ?ש?\"+name))\n",
    "            rgx_list.append(str.format(\"ה?\"+name))\n",
    "            rgx_list.append(str.format(\"ב?\"+name))\n",
    "            rgx_list.append(str.format(\"מ?\"+name))\n",
    "            rgx_list.append(str.format(\"ש?\"+name))\n",
    "            rgx_list.append(str.format(\"כ?\"+name))\n",
    "            rgx_list.append(name)\n",
    "            \n",
    "            if len(name.split()) > 1: \n",
    "                rgx_list.append(str.format(name.split()[0] + '-' + name.split()[1]))\n",
    "                \n",
    "            if len(name.split('-')) > 1:\n",
    "                rgx_list.append(str.format(name.split('-')[0] + ' ' + name.split('-')[1]))\n",
    "           \n",
    "        for rgx_match in rgx_list:\n",
    "            \n",
    "            # take only specific words - not substrings, \\\\b is the word border.\n",
    "            pattern = re.compile(str.format(\"\\\\b{0}\\\\b\",rgx_match))\n",
    "            updated_name = re.sub(pattern,replace_with_this_str,updated_df_element)\n",
    "    \n",
    "    after_elimination = ' '.join(updated_name.split())\n",
    "    \n",
    "    return after_elimination\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminate empty and duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminiate_empty_and_duplicates(df,column:str):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    eliminates empty rows and duplicated rows of a specific column of the df\n",
    "    \n",
    "    df - Data Frame to work on\n",
    "    \n",
    "    column - column name\n",
    "    \n",
    "    returns the df after the percedure\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    new_df = df.copy()\n",
    "   \n",
    "    new_df[new_df[column].astype(bool)] \n",
    "    print(str.format('After eliminating empty {0}',new_df.shape))\n",
    "    \n",
    "    new_df.drop_duplicates(subset=column,keep=False,inplace=True)\n",
    "    print(str.format('After clearing duplicates {0}',new_df.shape))\n",
    "    \n",
    "    new_df.reset_index(drop=True,inplace=False)\n",
    "   \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating lewenstein distance based method to eliminate 2 chars apart string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_identical(input_word:str,input_collection,mismatch_count):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    check if specified word has mismatch_count amount of chars difference from any of the words in the collection\n",
    "    \n",
    "    input_word - string \n",
    "    \n",
    "    input_collection - collection that stores strings to check upon\n",
    "    \n",
    "    mismatch_count - the maximal difference between the input word and the matched one\n",
    "    \n",
    "    retuns True for a match and False for a mismatch\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for item in input_collection:\n",
    "        if jellyfish.levenshtein_distance(word,item) <= mismatch_count:\n",
    "            return True\n",
    "    return False\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminating verdic naming common uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliiminates_naming(text:str,path):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Eliminates all names match that has been on the csv file\n",
    "    \n",
    "    text - the string to work on\n",
    "    \n",
    "    path - the csv file path\n",
    "    \n",
    "    returns the string after eliminating all matched words\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    with open(path,\"r\",encoding=\"utf-8\") as csv_name:\n",
    "        csv_names = csv.reader(csv_name,delimiter=',')\n",
    "        names = [row[0] for row in csv_names]\n",
    "        after_names = ' '.join([word for word in str(text).split() if word not in names])\n",
    "    \n",
    "    return after_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eliiminates_naming(input_names:list,path):\n",
    "    \n",
    "#     \"\"\"\n",
    "    \n",
    "#     Eliminates all names match that has been on the csv file\n",
    "    \n",
    "#     input_names - a list containing the strings to work on\n",
    "    \n",
    "#     path - the csv file path\n",
    "    \n",
    "#     returnes the new list after eliminating all matched words from the original strings\n",
    "    \n",
    "#     \"\"\"\n",
    "    \n",
    "#     new_list = list()\n",
    "#     with open(path,\"r\",encoding=\"utf-8\") as csv_name:\n",
    "#         csv_names = csv.reader(csv_name,delimiter=',')\n",
    "#         names = [row[0] for row in csv_names]\n",
    "#         for name in input_names:\n",
    "#             after_names = ' '.join([word for word in str(name).split() if word not in names])\n",
    "#             if after_names != '':\n",
    "#                 new_list.append(after_names)\n",
    "            \n",
    "#     return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminating unwanted chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliiminates_unwanted_chars(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    That method is in charge of some of the pre-process precedures:\n",
    "    \n",
    "    1. eliminates pharenthesis  \n",
    "    2. eliminate id numbers\n",
    "    3. minimize spaces on last names\n",
    "    \n",
    "    text - the string we work on\n",
    "    \n",
    "    returns the text after the process.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for word in str(text).split(' '):\n",
    "        if word.find(\"(\") != -1 or word.find(\")\") != -1:\n",
    "            text = text.replace(word,\"\")\n",
    "        if word.find('.') != -1:\n",
    "            for char in word:\n",
    "                if char in string.digits:\n",
    "                    text = text.replace(word,\"\")\n",
    "                    break\n",
    "                    \n",
    "    text = '-'.join(text.split(' - '))\n",
    "            \n",
    "    text = ' '.join([word for word in text.split() if word != ' '])\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling multinames rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df_to_proper_name_and_multiname(df,column:str):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    splits given df to two:\n",
    "    \n",
    "    first part is names that has maximal value of 3, first name and two last names or less.\n",
    "    \n",
    "    second part are names that containes more than 3 strings\n",
    "    \n",
    "    df - the data frame to work on\n",
    "    \n",
    "    column - the string describing the desired column\n",
    "    \n",
    "    returns the original df as the first value and the new one as the second, \n",
    "    \n",
    "    when the original now contains only \"good\" values and the second only \"problematic\" ones\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # initialization\n",
    "    df.reset_index(inplace=True,drop=True)\n",
    "    indexes = list()\n",
    "    new_values = list()\n",
    "    \n",
    "    # drop all the rows of the specified column\n",
    "    df = df.dropna(subset=[column])\n",
    "    \n",
    "    # iterate on each row as string\n",
    "    for i, presplit in enumerate(df[column].astype(str)):\n",
    "        \n",
    "        # split and check how many strings exists\n",
    "        values = presplit.split(' ')\n",
    "        if len(values) > 3:\n",
    "            \n",
    "            # append the index and the whole string\n",
    "            indexes.append(i)\n",
    "            new_values.append(presplit)\n",
    "    \n",
    "    # copy all the values that has been found as \"problematic\" ones to a new df\n",
    "    new_df = df.iloc[indexes, :].copy()\n",
    "    \n",
    "    # populate the new df with the values\n",
    "    new_df[column] = new_values\n",
    "    \n",
    "    # drop the problematic indexes from the original df and reset indexes on both of the dfs\n",
    "    df.drop(df.index[indexes],inplace=True)\n",
    "    df.reset_index(inplace=True,drop=True)\n",
    "    new_df.reset_index(inplace=True,drop=True)\n",
    "    \n",
    "    return df,new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling un-proper names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orginize_name(df,column:str):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    deals with names rows that contain more the 3 strings\n",
    "    \n",
    "    the mechanism handles two secnarios sperately:\n",
    "    \n",
    "    1. when then amount of strings is precisely 3, \n",
    "       it fixes the first string to be the first name and both second and third to be last name seperated by '-'\n",
    "    \n",
    "    2. even amount of strings - iterate and add couple of strings as first name and last name\n",
    "    \n",
    "    assumptions:\n",
    "    \n",
    "    all names has already passed threw the preprocess - thus, last name that contains '-' is present without any spaces.\n",
    "    \n",
    "    there will be misidentified strings on that process, it relies on the parsed data.\n",
    "    \n",
    "    df - the data frame to work on\n",
    "    \n",
    "    column - the string describing the column \n",
    "    \n",
    "    returns the same df after the process\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # initialization\n",
    "    new_values=list()\n",
    "    indexes = list()\n",
    "    \n",
    "    # iterate on each row as string\n",
    "    for i, presplit in enumerate(df[column].astype(str)):\n",
    "        values = ' '.join(presplit.split()).split()\n",
    "        \n",
    "        # if the len is precisely 3, fix first name and two last names seperated by '-'\n",
    "        if len(values) == 3:\n",
    "            full_name = str()\n",
    "            first_name = values[0]\n",
    "            values.remove(values[0])\n",
    "            last_name = '-'.join(values)\n",
    "            if first_name.find('-') == -1:\n",
    "                full_name = str.format('{0} {1}',first_name,last_name)\n",
    "            else:\n",
    "                full_name = str.format('{0} {1}',last_name,first_name)\n",
    "            \n",
    "            df.at[i,column] = full_name\n",
    "        \n",
    "        # if the len > 2 and even, iterate and assign couples until the string is empty \n",
    "        if len(values) > 2 and len(values) % 2 == 0:\n",
    "            for idx in range(int(len(values)//2)):\n",
    "                splitted=str()\n",
    "                if values[0].find('-') == -1:\n",
    "                    splitted = str.format(\"{0} {1}\",values[2*idx],values[(2*idx)+1])\n",
    "                \n",
    "                # that scenario tries to handle cases when last names comes before first ofir-buzaglo itai instead of itai ofir-buzaglo\n",
    "                else:\n",
    "                    splitted = str.format(\"{0} {1}\",values[2*idx+1],values[(2*idx)])\n",
    "                \n",
    "                # if the index equal to 0 the series already exists, just assign\n",
    "                if idx == 0:\n",
    "                    df.loc[i,column] = splitted\n",
    "                \n",
    "                # if the index is greated than 0, hence we must append new series.\n",
    "                elif idx > 0:\n",
    "                    cur_series = pd.Series(df.loc[i,:])\n",
    "                    cur_series.at[column] = splitted\n",
    "                    df.append(cur_series)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort the attorney and judges by frequency and detrmine naming convention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_normalized_names(df,path:str,roles:list):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    counts all occurences of the names exists on the data frame after the preprocess in the whole available verdicts derictory\n",
    "    \n",
    "    assumptions:\n",
    "    1. the right form of the name is majorly present\n",
    "\n",
    "    df - the data frame to work on\n",
    "    \n",
    "    path - the path that holds all the vericts jsons available \n",
    "    \n",
    "    roles - a list of roles to be examined.\n",
    "            Attention! use precisely json scheme name here, for example: העותר\n",
    "            \n",
    "    returns the df after counters assignment\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialization\n",
    "    df[\"Count_Full_Name\"] = 0\n",
    "    df[\"Count_First_Name\"] = 0\n",
    "    df[\"Count_Last_Name\"] = 0\n",
    "\n",
    "    # count the amount of files present\n",
    "    only_files = next(os.walk(path))[2]\n",
    "    total=len(only_files)\n",
    "    \n",
    "    # iterate on the files\n",
    "    for count,file in enumerate(os.scandir(path)):       \n",
    "        with open(file,\"r\",encoding='utf-8') as json_file:\n",
    "            verdict_json = json.load(json_file)\n",
    "            for role in roles:\n",
    "                \n",
    "                # check for match and advance counter accordingly.\n",
    "                # look for a match between all the roles given\n",
    "        \n",
    "                cur_verdict = ' '.join(verdict_json[\"_source\"][\"doc\"][\"Doc Details\"][role])\n",
    "                for idx,row in enumerate(df['Full_Name'].astype(str)):\n",
    "                    if row in cur_verdict:\n",
    "                        df.at[idx,'Count_Full_Name'] += 1\n",
    "                        cur_verdict = cur_verdict.replace(row,'')\n",
    "                    if not cur_verdict.strip():\n",
    "                        break\n",
    "                \n",
    "                cur_verdict = ' '.join(verdict_json[\"_source\"][\"doc\"][\"Doc Details\"][role])        \n",
    "                for idx,row in enumerate(df['First_Name'].astype(str)):\n",
    "                    if row in cur_verdict:\n",
    "                        df.at[idx,'Count_First_Name'] += 1\n",
    "                        cur_verdict = cur_verdict.replace(row,'')\n",
    "                    if not cur_verdict.strip():\n",
    "                        break\n",
    "                \n",
    "                cur_verdict = ' '.join(verdict_json[\"_source\"][\"doc\"][\"Doc Details\"][role])        \n",
    "                for idx,row in enumerate(df['Last_Name'].astype(str)):\n",
    "                    if row in cur_verdict:\n",
    "                        df.at[idx,'Count_Last_Name'] += 1\n",
    "                        cur_verdict = cur_verdict.replace(row,'')\n",
    "                    if not cur_verdict.strip():\n",
    "                        break\n",
    "        \n",
    "        #print each 10000 files\n",
    "        if count % 10000 == 0:\n",
    "            print(str.format(\"{0}/{1}\",count,total))\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidy_split(df, column, sep='|', keep=False):\n",
    "    \n",
    "    # credit: https://github.com/cognoma/genes/blob/721204091a96e55de6dcad165d6d8265e67e2a48/2.process.py#L61-L95\n",
    "    \n",
    "    \"\"\"\n",
    "    Split the values of a column and expand so the new DataFrame has one split\n",
    "    value per row. Filters rows where the column is missing.\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    df : pandas.DataFrame\n",
    "        dataframe with the column to split and expand\n",
    "    column : str\n",
    "        the column to split and expand\n",
    "    sep : str\n",
    "        the string used to split the column's values\n",
    "    keep : bool\n",
    "        whether to retain the presplit value as it's own row\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Returns a dataframe with the same columns as `df`.\n",
    "    \"\"\"\n",
    "    indexes = list()\n",
    "    new_values = list()\n",
    "    df = df.dropna(subset=[column])\n",
    "    for i, presplit in enumerate(df[column].astype(str)):\n",
    "        values = presplit.split(sep)\n",
    "        if keep and len(values) > 1:\n",
    "            indexes.append(i)\n",
    "            new_values.append(presplit)\n",
    "        for value in values:\n",
    "            indexes.append(i)\n",
    "            new_values.append(value)\n",
    "    new_df = df.iloc[indexes, :].copy()\n",
    "    new_df[column] = new_values\n",
    "    new_df.reset_index(inplace=True,drop=True)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path = 'C:/Users/Itai/HebrewCourtVerdictsAnalyzer/ML/data/normalization'\n",
    "df = create_naming_df(path)\n",
    "print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column='Full_Name'\n",
    "df[column] =df[\"Before\"].apply(lambda x :  eliiminates_unwanted_chars(x))\n",
    "print(\"Eliminates unwanted\" + str(df.shape))\n",
    "\n",
    "print()\n",
    "print(type(df))\n",
    "print(df.describe())\n",
    "print()\n",
    "\n",
    "options = ['defense', 'petitioner']  \n",
    "not_legal_df = df.loc[df['Role'].isin(options)]\n",
    "\n",
    "print()\n",
    "print(type(not_legal_df))\n",
    "print(not_legal_df.describe())\n",
    "print()\n",
    "\n",
    "not_legal_index_names = df[ (df['Role'] == 'defense') | (df['Role'] == 'petitioner') ].index\n",
    "df.drop(not_legal_index_names,inplace=True)\n",
    "\n",
    "print()\n",
    "print(type(df))\n",
    "print(df.describe())\n",
    "print()\n",
    "\n",
    "df = tidy_split(df,column,sep=';')\n",
    "print(\"after split by ; \" + str(df.shape))\n",
    "\n",
    "df = tidy_split(df,column,sep=',')\n",
    "print(\"after split by , \" + str(df.shape))\n",
    "\n",
    "naming_path = 'C:/Users/Itai/HebrewCourtVerdictsAnalyzer/ML/naming_stopwords.csv'\n",
    "stopwords_path = 'C:/Users/Itai/HebrewCourtVerdictsAnalyzer/ML/stopwordsafterfilter.csv'\n",
    "\n",
    "df[column] = df[column].apply(lambda name_one : eliiminates_naming(name_one,naming_path))\n",
    "\n",
    "df[column] = df[column].apply(lambda name_two : eliiminates_naming(name_two,stopwords_path))\n",
    "\n",
    "df = orginize_name(df,column)\n",
    "print(df.shape)\n",
    "print(df.loc[:100,:])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_first_and_last_name(df,column:str):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    splits full name to first and last name\n",
    "    \n",
    "    df - the data frame to work on\n",
    "    \n",
    "    column - string describing the column name\n",
    "    \n",
    "    returns the df with new 'First_Name' and 'Last_Name' columns\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #initalization\n",
    "    indexes = []\n",
    "    first_names = []\n",
    "    last_names = []\n",
    "    \n",
    "    # iterate on each row and split, if first string contains '-', \n",
    "    # thus, it is last name and therefore we need to change posiotion\n",
    "    for idx,name in enumerate(df[column].astype(str)):\n",
    "        splitted = name.split()\n",
    "        if len(splitted) == 2:\n",
    "            indexes.append(idx)\n",
    "            if splitted[0].find('-') == -1:\n",
    "                first_names.append(splitted[0])\n",
    "                last_names.append(splitted[1])\n",
    "            else:\n",
    "                first_names.append(splitted[1])\n",
    "                last_names.append(splitted[0])\n",
    "    \n",
    "    # addign values to newly created column\n",
    "    df.loc[indexes,'First_Name'] = first_names\n",
    "    df.loc[indexes,'Last_Name'] = last_names\n",
    "    \n",
    "    print(str.format('Successfully splitted {0} names',len(indexes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_name_exists(full_name:str,txt_path:str):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    looks for a match with the supreme court judges txt file\n",
    "    \n",
    "    full_name - a string of the full name after the process\n",
    "    \n",
    "    text_path - a string with the txt file path\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if len(full_name.split()) == 2:\n",
    "        first_name = full_name.split()[0]\n",
    "        last_name = full_name.split()[1]\n",
    "        \n",
    "        with open(txt_path,'r') as text_file:\n",
    "            judges_list = text_file.readlines()\n",
    "            for cur in judges_list:\n",
    "                cur_first_name = cur.split()[0]\n",
    "                cur_last_name = cur.split()[1]\n",
    "                if cur_first_name[0] == first_name[0] and cur_last_name == last_name:\n",
    "                    return cur\n",
    "        \n",
    "    return full_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_single_name(name):\n",
    "    \n",
    "    new_name=str()\n",
    "    \n",
    "    if str(name).find('-') != -1:\n",
    "        count = 0\n",
    "        for char in name:\n",
    "            if char == '-':\n",
    "                count += 1\n",
    "                \n",
    "        if count != 1:\n",
    "            left =str()\n",
    "            right = str()\n",
    "            left = name.split('-')[0]\n",
    "            right = name.split('-')[1]\n",
    "            left = ''.join([char for char in str(left) if char not in string.punctuation])\n",
    "            right = ''.join([char for char in str(right) if char not in string.punctuation])\n",
    "            new_name = str.format('{0}-{1}',left,right)    \n",
    "    else:\n",
    "        new_name = ''.join([char for char in str(name) if char not in string.punctuation])\n",
    "    \n",
    "    return new_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clean_single_name('it.\\]['']ai')\n",
    "print( res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_first_and_last_name(df,'Full_Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['First_Name'] = df['First_Name'].apply(lambda first: clean_single_name(first))\n",
    "df['Last_Name'] = df['Last_Name'].apply(lambda last: clean_single_name(last))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = 'C:/Users/Itai/HebrewCourtVerdictsAnalyzer/ML/israel_supreme_court_judges.txt'\n",
    "df['Normalized'] = df['Full_Name'].apply(lambda normalized : check_if_name_exists(normalized,csv_path))\n",
    "df.loc[20:,'Full_Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = df[df['Normalized'] != np.nan]\n",
    "print (indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df.loc[1400:1450,['First_Name','Last_Name','Normalized']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(keep=False,inplace=True,ignore_index=True)\n",
    "df.dropna()\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "df.sort_values(by=['First_Name'],inplace=True)\n",
    "csv_path = 'C:/Users/Itai/HebrewCourtVerdictsAnalyzer/ML/legal_by_first.csv'\n",
    "df.to_csv(csv_path,index=False,encoding='utf-8')\n",
    "df.sort_values(by=['Last_Name'],inplace=True)\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "csv_path = 'C:/Users/Itai/HebrewCourtVerdictsAnalyzer/ML/legal_by_last.csv'\n",
    "df.to_csv(csv_path,index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_not_legal(names):\n",
    "    \n",
    "    processed_names =[eliiminates_unwanted_chars(name) for name in names]\n",
    "    \n",
    "    return processed_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before = ['איתי אופיר - אבולעפיה','איתי (אופיר) אופיר','תז.235 משה אופיר']\n",
    "after = pre_process_not_legal(before)\n",
    "print(after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_char(names,char):\n",
    "    \n",
    "    splitted = list()\n",
    "    \n",
    "    \n",
    "    for name in names:\n",
    "        \n",
    "        flag = False\n",
    "        \n",
    "        name_one = str()\n",
    "        name_two = str()\n",
    "        \n",
    "        if name.find(char) != -1:\n",
    "            \n",
    "            name_one = ' '.join(name.split(char)[0].split())\n",
    "            name_two = ' '.join(name.split(char)[1].split())\n",
    "            \n",
    "            flag=True\n",
    "        \n",
    "        if flag:\n",
    "            splitted.append(name_one)\n",
    "            splitted.append(name_two)\n",
    "        else:\n",
    "            splitted.append(name)\n",
    "    \n",
    "        \n",
    "    return splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orginize_name(names):\n",
    "    \n",
    "    new_values=list()\n",
    "    for name in names:\n",
    "        values = ' '.join(name.split()).split()\n",
    "        \n",
    "        flag = False\n",
    "        \n",
    "        if len(values) == 3:\n",
    "            first_name = values[0]\n",
    "            values.remove(values[0])\n",
    "            last_name = '-'.join(values)\n",
    "            full_name = str.format('{0} {1}',first_name,last_name)\n",
    "            new_values.append(full_name)\n",
    "            \n",
    "            flag = True\n",
    "        \n",
    "        elif len(values) > 2 and len(values) % 2 == 0:\n",
    "            for idx in range(int(len(values)//2)):\n",
    "                splitted = str.format(\"{0} {1}\",values[2*idx],values[(2*idx)+1])\n",
    "                new_values.append(splitted)\n",
    "            flag = True\n",
    "        \n",
    "        if not flag:\n",
    "            new_values = names\n",
    "        \n",
    "    return new_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_order(full_name:str):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    determines if specified full name is in the correct order: (first name) (last_name)\n",
    "    \n",
    "    full_name - string to be evaluated\n",
    "    \n",
    "    returns the string after order is set\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # initalization\n",
    "    new_str = str()\n",
    "    \n",
    "    # split the string\n",
    "    values = full_name.split()\n",
    "    first = values[0]\n",
    "    last = values[1]\n",
    "    \n",
    "    # check if the first name contains '-' - that means it is last name in thw rong position\n",
    "    if first.find('-') != -1:\n",
    "        new_str = f'last first'\n",
    "    else:\n",
    "        new_str = f'first last'\n",
    "    \n",
    "    return new_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before1 = ['איתי  אופיר,אורן   חזן','אריק איינשטיין,גוזי כץ']\n",
    "before2 = ['איתי  אופיר;אורן   חזן','אריק איינשטיין;גוזי כץ']\n",
    "before3 = ['איתי אופיר']\n",
    "after1 = split_by_char(before1,',')\n",
    "after2 = split_by_char(before2,';')\n",
    "after3 = split_by_char(before3,';')\n",
    "\n",
    "print(after1)\n",
    "print(after2)\n",
    "print(after3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before1 = ['איתי אופיר אייזק ניוטון משה לוי','איתי אופיר אבולעפיה']\n",
    "before2 = ['איתי-טוביה אופיר']\n",
    "\n",
    "after1 = orginize_name(before1)\n",
    "after2 = orginize_name(before2)\n",
    "\n",
    "print(after1)\n",
    "print(after2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_legal(names):\n",
    "    \n",
    "    processed_names = list()\n",
    "    \n",
    "    names_after_first_split = split_by_char(names,';')\n",
    "    names_after_second_split = split_by_char(names_after_first_split,',')\n",
    "    \n",
    "    naming_path = 'C:/Users/Itai/HebrewCourtVerdictsAnalyzer/ML/naming_stopwords.csv'\n",
    "    stopwords_path = 'C:/Users/Itai/HebrewCourtVerdictsAnalyzer/ML/stopwordsafterfilter.csv'\n",
    "\n",
    "    names_after_naming = [eliiminates_naming(name,naming_path) for name in names_after_second_split]\n",
    "    names_after_stopwords = [eliiminates_naming(name,stopwords_path) for name in names_after_naming]\n",
    "\n",
    "    names_ready_for_for_first_and_last_name = orginize_name(names_after_stopwords)\n",
    "    after_single_clean_list = list()\n",
    "    for name in names_ready_for_for_first_and_last_name:\n",
    "        after_single_clean_list.append(clean_single_name(name))\n",
    "    \n",
    "    # add pd.Series addition to main DataFrame here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_normalized_values_to_json(verdict_path,dest_path,input_list,new_role_key):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    adds the normalized key with the new values to the json and write it to destination\n",
    "    \n",
    "    verdict_path - the string of the verdict path\n",
    "    \n",
    "    dest_path - the string of the destination directory\n",
    "    \n",
    "    input_list - the normalized values strings in a list\n",
    "    \n",
    "    new_role_key - the key string that will be added to the json\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    with open(verdict_path,'r') as json_file:\n",
    "        \n",
    "        verdict = json.load(json_file)\n",
    "        verdict['_source']['doc']['Doc Details'][role] = input_list\n",
    "        verdict_id = verdict[\"_id\"]\n",
    "        path = dest_path + '/' + verdict_id\n",
    "        \n",
    "        with open(path,'w') as normalized_json:\n",
    "            json.dump(verdict,normalized_json,ensure_ascii=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(json_path,output_path):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    takes the json path and path all the representitives names thru the precedure\n",
    "    \n",
    "    json_path - the verdict path string\n",
    "    \n",
    "    output_path - the string of the desired location to write the new json with the normalized names\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    cleaned_petitioners = list()\n",
    "    cleaned_defedse = list()\n",
    "    cleaned_judges = list()\n",
    "    cleaned_petitioners_attorneys = list()\n",
    "    cleaned_defendse_attorneys = list()\n",
    "    \n",
    "    with open(json_path,'r',encoding='utf-8')as json_file:\n",
    "        \n",
    "        verdict_json = json.load(json_file)\n",
    "        \n",
    "        petitioners = verdict_json[\"_source\"][\"doc\"][\"Doc Details\"][\"העותר\"]\n",
    "        defense = verdict_json[\"_source\"][\"doc\"][\"Doc Details\"][\"המשיב\"]\n",
    "        \n",
    "        judges = verdict_json[\"_source\"][\"doc\"][\"Doc Details\"][\"לפני\"]\n",
    "        petitioner_attorneys = verdict_json[\"_source\"][\"doc\"][\"Doc Details\"][\"בשם העותר\"]\n",
    "        defendse_attorney = verdict_json[\"_source\"][\"doc\"][\"Doc Details\"][\"בשם המשיב\"]\n",
    "                \n",
    "        for petitioner in petitioners:\n",
    "            cleaned_petitioners.append(pre_process_not_legal(petitioner))\n",
    "            \n",
    "        for defender in defense:\n",
    "            cleaned_defedse.append(pre_process_not_legal(defendee))\n",
    "        \n",
    "        for judge in judges:\n",
    "            cleaned_judges.append(pre_process_legal(judge))\n",
    "        \n",
    "        for petitioner_attorney in petitioner_attorneys:\n",
    "            cleaned_petitioners_attorneys.append(pre_process_legal(petitioner_attorney))\n",
    "            \n",
    "        for defendse_attorney in cleaned_defendse_attorneys:\n",
    "            cleaned_defendse_attorneys.append(pre_process_legal(defendse_attorney))\n",
    "            \n",
    "        write_normalized_values_to_json(json_path,output_path,cleaned_petitioners,'העותר מנורמל')\n",
    "        write_normalized_values_to_json(json_path,output_path,cleaned_defedse,'המשיב מנורמל')\n",
    "        write_normalized_values_to_json(json_path,output_path,cleaned_judges,'לפני מנורמל')\n",
    "        write_normalized_values_to_json(json_path,output_path,cleaned_petitioners_attorneys,'בשם העותר מנורמל')\n",
    "        write_normalized_values_to_json(json_path,output_path,cleaned_defendse_attorneys,'בשם המשיב מנורמל')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.append(not_legal_df,ignore_index=True)\n",
    "# print(df.shape)\n",
    "# df.reset_index(inplace=True,drop=True)\n",
    "# print(df.loc[:50,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sort_values(by=['After'],ascending=False,inplace=True)\n",
    "# df.reset_index(inplace=True,drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shrinked_df = df.sample(10)\n",
    "# path = 'C:/Users/Itai/HebrewCourtVerdictsAnalyzer/ML/data/splitteredjsonsaftercorrection'\n",
    "# names = [\"בשם העותר\",\"בשם המשיב\",\"לפני\",\"העותר\",\"המשיב\"]\n",
    "# df = count_normalized_names(shrinked_df,column,path,names)\n",
    "# csv_path = 'C:/Users/Itai/HebrewCourtVerdictsAnalyzer/ML/normalized_function.csv'\n",
    "# df.to_csv(csv_path,index=False,encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
